{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rhGsJx97Tzq"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "class ParallelLanguageDataset(Dataset):\n",
        "    def __init__(self, data_path_1, data_path_2, tokenizer, max_len):\n",
        "        self.data_1, self.data_2 = self.load_data(data_path_1, data_path_2)\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_1)\n",
        "\n",
        "    def __getitem__(self, item_idx):\n",
        "        sent1 = str(self.data_1[item_idx])\n",
        "        sent2 = str(self.data_2[item_idx])\n",
        "        encoded_output_sent1 = self.tokenizer.encode_plus(\n",
        "            sent1,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True\n",
        "        )\n",
        "        encoded_output_sent2 = self.tokenizer.encode_plus(\n",
        "            sent2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        encoded_output_sent1[\"attention_mask\"][\n",
        "            encoded_output_sent1[\"attention_mask\"] == 1\n",
        "        ] = 2\n",
        "        encoded_output_sent1[\"attention_mask\"][\n",
        "            encoded_output_sent1[\"attention_mask\"] == 0\n",
        "        ] = True\n",
        "        encoded_output_sent1[\"attention_mask\"][\n",
        "            encoded_output_sent1[\"attention_mask\"] == 2\n",
        "        ] = False\n",
        "        encoded_output_sent1[\"attention_mask\"] = encoded_output_sent1[\n",
        "            \"attention_mask\"\n",
        "        ].type(torch.bool)\n",
        "\n",
        "        encoded_output_sent2[\"attention_mask\"][\n",
        "            encoded_output_sent2[\"attention_mask\"] == 1\n",
        "        ] = 2\n",
        "        encoded_output_sent2[\"attention_mask\"][\n",
        "            encoded_output_sent2[\"attention_mask\"] == 0\n",
        "        ] = True\n",
        "        encoded_output_sent2[\"attention_mask\"][\n",
        "            encoded_output_sent2[\"attention_mask\"] == 2\n",
        "        ] = False\n",
        "        encoded_output_sent2[\"attention_mask\"] = encoded_output_sent2[\n",
        "            \"attention_mask\"\n",
        "        ].type(torch.bool)\n",
        "\n",
        "        return_dict = {\n",
        "            \"ids1\": encoded_output_sent1[\"input_ids\"].flatten(),\n",
        "            \"ids2\": encoded_output_sent2[\"input_ids\"].flatten(),\n",
        "            \"masks_sent1\": encoded_output_sent1[\"attention_mask\"].flatten(),\n",
        "            \"masks_sent2\": encoded_output_sent2[\"attention_mask\"].flatten(),\n",
        "        }\n",
        "        return return_dict\n",
        "\n",
        "    def load_data(self, data_path_1, data_path_2):\n",
        "        with open(data_path_1, \"r\") as f:\n",
        "            data_1 = f.read().splitlines()[1:200]\n",
        "        with open(data_path_2, \"r\") as f:\n",
        "            data_2 = f.read().splitlines()[1:200]\n",
        "        return data_1, data_2\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpS8XtEXaeUN",
        "outputId": "fa99d573-7924-4c5b-b4e7-deed0aadd180"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSy3zPhkya8w"
      },
      "source": [
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LanguageTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model,\n",
        "        nhead,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dim_feedforward,\n",
        "        max_seq_length,\n",
        "        pos_dropout,\n",
        "        trans_dropout,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed_src = nn.Embedding(vocab_size, d_model)\n",
        "        self.embed_tgt = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model,\n",
        "            nhead,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            dim_feedforward,\n",
        "            trans_dropout,\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        tgt,\n",
        "        src_key_padding_mask,\n",
        "        tgt_key_padding_mask,\n",
        "        memory_key_padding_mask,\n",
        "        tgt_mask,\n",
        "    ):\n",
        "        src = rearrange(src, \"n s -> s n\")\n",
        "        tgt = rearrange(tgt, \"n t -> t n\")\n",
        "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))\n",
        "        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        output = self.transformer(\n",
        "            src,\n",
        "            tgt,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask,\n",
        "        )\n",
        "        output = rearrange(output, \"t n e -> n t e\")\n",
        "        return self.fc(output)\n",
        "\n",
        "\n",
        "# Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnTOmBAK9q18",
        "outputId": "470cc2aa-6838-4aff-a7fb-f7c42f21603b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l8eus1w94ss",
        "outputId": "0120a2ff-3312-4528-e3e6-e0b221f3afc8"
      },
      "source": [
        "cd /content/drive/MyDrive/Data/"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkNqEAV-YVg4"
      },
      "source": [
        "'''A wrapper class for optimizer '''\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# From https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Optim.py\n",
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients by the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "            "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liYISQaOanj7",
        "outputId": "85f7b7c7-8f70-46c3-8df9-31861ee19469"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2mnDVXrdtX4"
      },
      "source": [
        "def train(train_loader, valid_loader, model, optim, criterion, num_epochs):\n",
        "    print_every = 5\n",
        "    model.train()\n",
        "\n",
        "    lowest_val = 1e9\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    total_step = 0\n",
        "    print(\"-\" * 100)\n",
        "    print(\"Starting Training\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm(total=print_every, leave=False)\n",
        "        total_loss = 0\n",
        "        train_loss = 0\n",
        "\n",
        "        for step, data_dict in enumerate(iter(train_loader)):\n",
        "            total_step += 1\n",
        "            src, tgt, src_key_padding_mask, tgt_key_padding_mask = (\n",
        "                data_dict[\"ids1\"],\n",
        "                data_dict[\"ids2\"],\n",
        "                data_dict[\"masks_sent1\"],\n",
        "                data_dict[\"masks_sent2\"],\n",
        "            )\n",
        "            src, src_key_padding_mask = src.to(\"cuda\"), src_key_padding_mask.to(\"cuda\")\n",
        "            tgt, tgt_key_padding_mask = tgt.to(\"cuda\"), tgt_key_padding_mask.to(\"cuda\")\n",
        "\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "            tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(\"cuda\")\n",
        "\n",
        "            optim.zero_grad()\n",
        "            outputs = model(\n",
        "                src,\n",
        "                tgt_inp,\n",
        "                src_key_padding_mask,\n",
        "                tgt_key_padding_mask[:, :-1],\n",
        "                memory_key_padding_mask,\n",
        "                tgt_mask,\n",
        "            )\n",
        "            loss = criterion(\n",
        "                rearrange(outputs, \"b t v -> (b t) v\"),\n",
        "                rearrange(tgt_out, \"b o -> (b o)\"),\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step_and_update_lr()\n",
        "            train_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            # train_losses.append((step, loss.item()))\n",
        "            pbar.update(1)\n",
        "            if step % print_every == print_every - 1:\n",
        "                pbar.close()\n",
        "                print(\n",
        "                    f\"Epoch [{epoch + 1} / {num_epochs}] \\t Step [{step + 1} / {len(train_loader)}] \\t \"\n",
        "                    f\"Train Loss: {total_loss / print_every}\"\n",
        "                )\n",
        "                total_loss = 0\n",
        "                pbar = tqdm(total=print_every, leave=False)\n",
        "        pbar.close()\n",
        "        val_loss = validate(valid_loader, model, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        train_losses.append(train_loss/len(train_loader))\n",
        "        if val_loss < lowest_val:\n",
        "            lowest_val = val_loss\n",
        "            torch.save(model, \"Output/transformer.pth\")\n",
        "        print(f\"Val Loss: {val_loss}\")\n",
        "    return train_losses, val_losses"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPwkx0QuduZf"
      },
      "source": [
        "def validate(valid_loader, model, criterion):\n",
        "    pbar = tqdm(total=len(iter(valid_loader)), leave=False)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    for step, data_dict in enumerate(iter(valid_loader)):\n",
        "        with torch.no_grad():\n",
        "            src, tgt, src_key_padding_mask, tgt_key_padding_mask = (\n",
        "                data_dict[\"ids1\"],\n",
        "                data_dict[\"ids2\"],\n",
        "                data_dict[\"masks_sent1\"],\n",
        "                data_dict[\"masks_sent2\"],\n",
        "            )\n",
        "            src, src_key_padding_mask = src.to(\"cuda\"), src_key_padding_mask.to(\"cuda\")\n",
        "            tgt, tgt_key_padding_mask = tgt.to(\"cuda\"), tgt_key_padding_mask.to(\"cuda\")\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            tgt_inp = tgt[:, :-1]\n",
        "            tgt_out = tgt[:, 1:].contiguous()\n",
        "            tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(\"cuda\")\n",
        "            outputs = model(\n",
        "                src,\n",
        "                tgt_inp,\n",
        "                src_key_padding_mask,\n",
        "                tgt_key_padding_mask[:, :-1],\n",
        "                memory_key_padding_mask,\n",
        "                tgt_mask,\n",
        "            )\n",
        "            loss = criterion(\n",
        "                rearrange(outputs, \"b t v -> (b t) v\"),\n",
        "                rearrange(tgt_out, \"b o -> (b o)\"),\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    model.train()\n",
        "    return total_loss / len(valid_loader)\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXHQEpwTekNm"
      },
      "source": [
        "def gen_nopeek_mask(length):\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, \"h w -> w h\")\n",
        "    mask = (\n",
        "        mask.float()\n",
        "        .masked_fill(mask == 0, float(\"-inf\"))\n",
        "        .masked_fill(mask == 1, float(0.0))\n",
        "    )\n",
        "    return mask\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IikNUlK6uKA3"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from pathlib import Path\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "from transformers import RobertaTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "num_epochs = 20\n",
        "batch_size = 8\n",
        "max_seq_length = 96\n",
        "d_model = 512\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "dim_feedforward = 2048\n",
        "nhead = 8\n",
        "pos_dropout = 0.1\n",
        "trans_dropout = 0.1\n",
        "n_warmup_steps = 2000\n",
        "PRE_TRAINED_MODEL_NAME = \"distilroberta-base\"\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1cjcVpNhz6M",
        "outputId": "65f1a20a-4250-4907-bda1-b9e742796ef7"
      },
      "source": [
        "\n",
        "project_path = \"\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "train_dataset = ParallelLanguageDataset(\n",
        "        project_path + \"raw/es/train.txt\",\n",
        "        project_path + \"raw/en/train.txt\",\n",
        "        tokenizer,\n",
        "        max_seq_length,\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "valid_dataset = ParallelLanguageDataset(\n",
        "        project_path + \"raw/es/val.txt\",\n",
        "        project_path + \"raw/en/val.txt\",\n",
        "        tokenizer,\n",
        "        max_seq_length,\n",
        "    )\n",
        "valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "model = LanguageTransformer(\n",
        "        tokenizer.vocab_size,\n",
        "        d_model,\n",
        "        nhead,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dim_feedforward,\n",
        "        max_seq_length,\n",
        "        pos_dropout,\n",
        "        trans_dropout,\n",
        "  ).to(\"cuda\")\n",
        "for p in model.parameters():\n",
        "  if p.dim() > 1:\n",
        "    nn.init.xavier_normal_(p)\n",
        "\n",
        "optim = ScheduledOptim(Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09), d_model, n_warmup_steps)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "train_losses, val_losses = train(\n",
        "      train_loader, valid_loader, model, optim, criterion, num_epochs\n",
        ")\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Starting Training\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.68it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 30] \t Step [5 / 25] \t Train Loss: 10.68599624633789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 30] \t Step [10 / 25] \t Train Loss: 10.469682693481445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.06it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 30] \t Step [15 / 25] \t Train Loss: 10.191908645629884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 30] \t Step [20 / 25] \t Train Loss: 9.875433540344238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 30] \t Step [25 / 25] \t Train Loss: 9.808604049682618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  9.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 11.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 13.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 14.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 9.517433280944823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2 / 30] \t Step [5 / 25] \t Train Loss: 9.451534652709961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2 / 30] \t Step [10 / 25] \t Train Loss: 9.2943359375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2 / 30] \t Step [15 / 25] \t Train Loss: 9.343553161621093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.68it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2 / 30] \t Step [20 / 25] \t Train Loss: 9.029743957519532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2 / 30] \t Step [25 / 25] \t Train Loss: 8.98768367767334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.28it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  9.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 10.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 12.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 14.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 8.962870979309082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.45it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3 / 30] \t Step [5 / 25] \t Train Loss: 8.801809692382813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3 / 30] \t Step [10 / 25] \t Train Loss: 8.778602600097656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3 / 30] \t Step [15 / 25] \t Train Loss: 8.699011993408202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3 / 30] \t Step [20 / 25] \t Train Loss: 8.699715042114258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3 / 30] \t Step [25 / 25] \t Train Loss: 8.214697933197021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  5.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  9.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 10.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 12.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 14.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 8.367808494567871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [4 / 30] \t Step [5 / 25] \t Train Loss: 8.29446496963501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.34it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [4 / 30] \t Step [10 / 25] \t Train Loss: 8.25817937850952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [4 / 30] \t Step [15 / 25] \t Train Loss: 7.663263320922852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.84it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [4 / 30] \t Step [20 / 25] \t Train Loss: 7.677864742279053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [4 / 30] \t Step [25 / 25] \t Train Loss: 7.509585285186768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  6.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  9.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 11.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 13.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 14.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 7.5112752342224125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5 / 30] \t Step [5 / 25] \t Train Loss: 7.412296867370605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5 / 30] \t Step [10 / 25] \t Train Loss: 7.093937492370605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.93it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5 / 30] \t Step [15 / 25] \t Train Loss: 7.082753276824951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.98it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5 / 30] \t Step [20 / 25] \t Train Loss: 6.464455509185791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5 / 30] \t Step [25 / 25] \t Train Loss: 6.2652593612670895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  5.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01,  8.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 10.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 12.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 14.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 6.4469485855102535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.96it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [6 / 30] \t Step [5 / 25] \t Train Loss: 6.226389217376709\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.04it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [6 / 30] \t Step [10 / 25] \t Train Loss: 6.039660167694092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.00it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [6 / 30] \t Step [15 / 25] \t Train Loss: 5.66365327835083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.96it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [6 / 30] \t Step [20 / 25] \t Train Loss: 5.3453185081481935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [6 / 30] \t Step [25 / 25] \t Train Loss: 5.270661926269531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:07,  3.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  8.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 10.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 12.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 14.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 5.3805584144592284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [7 / 30] \t Step [5 / 25] \t Train Loss: 4.876275062561035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.00it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [7 / 30] \t Step [10 / 25] \t Train Loss: 4.775540542602539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.69it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [7 / 30] \t Step [15 / 25] \t Train Loss: 4.784433841705322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.97it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [7 / 30] \t Step [20 / 25] \t Train Loss: 4.339584159851074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [7 / 30] \t Step [25 / 25] \t Train Loss: 4.48900671005249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:11,  2.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:08,  2.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:05,  3.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  4.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  6.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:01<00:01,  7.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01,  9.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 11.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 19/25 [00:01<00:00, 12.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 22/25 [00:01<00:00, 14.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 25/25 [00:01<00:00, 16.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 4.563209838867188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8 / 30] \t Step [5 / 25] \t Train Loss: 4.117090129852295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.04it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8 / 30] \t Step [10 / 25] \t Train Loss: 3.8176300048828127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8 / 30] \t Step [15 / 25] \t Train Loss: 4.016630697250366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8 / 30] \t Step [20 / 25] \t Train Loss: 3.5987485885620116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8 / 30] \t Step [25 / 25] \t Train Loss: 4.0885828018188475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:12,  1.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 2/25 [00:00<00:09,  2.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:05,  3.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:03,  4.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:01<00:02,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:01<00:01,  7.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  8.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 10.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 12.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 14.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 4.1339310359954835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9 / 30] \t Step [5 / 25] \t Train Loss: 3.4582923889160155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.49it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9 / 30] \t Step [10 / 25] \t Train Loss: 3.5516398429870604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.59it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9 / 30] \t Step [15 / 25] \t Train Loss: 3.4514130115509034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.09it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9 / 30] \t Step [20 / 25] \t Train Loss: 3.442330741882324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9 / 30] \t Step [25 / 25] \t Train Loss: 3.590365934371948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  5.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  9.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 11.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 13.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 14.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.794139289855957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.62it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10 / 30] \t Step [5 / 25] \t Train Loss: 3.267364835739136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.94it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10 / 30] \t Step [10 / 25] \t Train Loss: 3.6003896236419677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10 / 30] \t Step [15 / 25] \t Train Loss: 3.2335607528686525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.93it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10 / 30] \t Step [20 / 25] \t Train Loss: 3.0504252433776857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10 / 30] \t Step [25 / 25] \t Train Loss: 2.9193382978439333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:07,  3.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:01,  8.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 10.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 12.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 13.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 25/25 [00:01<00:00, 15.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.6221659564971924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.27it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11 / 30] \t Step [5 / 25] \t Train Loss: 3.071144676208496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11 / 30] \t Step [10 / 25] \t Train Loss: 3.0142813444137575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11 / 30] \t Step [15 / 25] \t Train Loss: 3.7246933937072755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11 / 30] \t Step [20 / 25] \t Train Loss: 2.78753457069397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11 / 30] \t Step [25 / 25] \t Train Loss: 2.7819424867630005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 300, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 224, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
            "    rmtree(tempdir)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 498, in rmtree\n",
            "    onerror(os.rmdir, path, sys.exc_info())\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 496, in rmtree\n",
            "    os.rmdir(path)\n",
            "OSError: [Errno 39] Directory not empty: '/tmp/pymp-cchil41h'\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  5.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 11/25 [00:00<00:01,  7.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01,  9.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 11.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 13.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 14.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.542877268791199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12 / 30] \t Step [5 / 25] \t Train Loss: 2.9713515758514406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12 / 30] \t Step [10 / 25] \t Train Loss: 3.0180198669433596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.69it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12 / 30] \t Step [15 / 25] \t Train Loss: 3.2236479759216308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12 / 30] \t Step [20 / 25] \t Train Loss: 2.794866609573364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12 / 30] \t Step [25 / 25] \t Train Loss: 2.960578441619873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:11,  2.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:07,  2.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:05,  3.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  4.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:02,  6.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:01<00:01,  7.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01,  9.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 11.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 12.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 14.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 16.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.506488733291626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13 / 30] \t Step [5 / 25] \t Train Loss: 2.7280697584152223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:00,  5.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.22it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13 / 30] \t Step [10 / 25] \t Train Loss: 3.23019118309021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13 / 30] \t Step [15 / 25] \t Train Loss: 2.8590651512145997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.98it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13 / 30] \t Step [20 / 25] \t Train Loss: 3.1053718090057374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13 / 30] \t Step [25 / 25] \t Train Loss: 2.7057403564453124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:03,  5.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 11/25 [00:00<00:01,  7.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01,  8.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 10.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 12.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 14.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.4827974128723143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14 / 30] \t Step [5 / 25] \t Train Loss: 2.410642147064209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14 / 30] \t Step [10 / 25] \t Train Loss: 3.114715003967285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14 / 30] \t Step [15 / 25] \t Train Loss: 3.2440606594085692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14 / 30] \t Step [20 / 25] \t Train Loss: 2.6261588096618653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14 / 30] \t Step [25 / 25] \t Train Loss: 3.0354193687438964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  5.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:02,  7.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:01<00:01,  9.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 11.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 12.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 14.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 16.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.4281315994262695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15 / 30] \t Step [5 / 25] \t Train Loss: 2.773270916938782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:00,  5.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15 / 30] \t Step [10 / 25] \t Train Loss: 2.754742908477783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15 / 30] \t Step [15 / 25] \t Train Loss: 3.0900034427642824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.28it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15 / 30] \t Step [20 / 25] \t Train Loss: 2.7065519094467163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15 / 30] \t Step [25 / 25] \t Train Loss: 2.7960651874542237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  7.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  9.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:00, 10.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 12.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 14.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 16.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.3902666664123533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.74it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16 / 30] \t Step [5 / 25] \t Train Loss: 2.7594011783599854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.99it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16 / 30] \t Step [10 / 25] \t Train Loss: 2.7939318895339964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16 / 30] \t Step [15 / 25] \t Train Loss: 2.4094712495803834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16 / 30] \t Step [20 / 25] \t Train Loss: 2.8484142780303956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16 / 30] \t Step [25 / 25] \t Train Loss: 2.942714786529541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:02,  7.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  9.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:00, 10.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 12.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 14.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 16.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.3399026679992674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17 / 30] \t Step [5 / 25] \t Train Loss: 2.9921896934509276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17 / 30] \t Step [10 / 25] \t Train Loss: 2.3470563173294066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17 / 30] \t Step [15 / 25] \t Train Loss: 2.5557615756988525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.92it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17 / 30] \t Step [20 / 25] \t Train Loss: 2.302156352996826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17 / 30] \t Step [25 / 25] \t Train Loss: 3.1065918445587157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:02,  7.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:01<00:01,  9.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:00, 10.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 12.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 14.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 15.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.288944597244263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  1.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18 / 30] \t Step [5 / 25] \t Train Loss: 2.5757070541381837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18 / 30] \t Step [10 / 25] \t Train Loss: 2.891794776916504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18 / 30] \t Step [15 / 25] \t Train Loss: 2.119794821739197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18 / 30] \t Step [20 / 25] \t Train Loss: 2.583682727813721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18 / 30] \t Step [25 / 25] \t Train Loss: 2.6015799045562744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:08,  2.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:03,  4.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  6.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  7.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01,  9.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 11.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 19/25 [00:01<00:00, 13.28it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 14.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 15.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 25/25 [00:01<00:00, 16.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.23463903427124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:05<00:20,  5.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:05<00:10,  3.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:05<00:05,  2.59s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:05<00:01,  1.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:05<00:00,  1.36s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19 / 30] \t Step [5 / 25] \t Train Loss: 2.2382635354995726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19 / 30] \t Step [10 / 25] \t Train Loss: 2.523200750350952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19 / 30] \t Step [15 / 25] \t Train Loss: 2.47236430644989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.06it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19 / 30] \t Step [20 / 25] \t Train Loss: 2.2783625602722166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19 / 30] \t Step [25 / 25] \t Train Loss: 2.6090713024139403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:12,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:08,  2.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:05,  3.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  4.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:01<00:02,  5.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 11/25 [00:01<00:01,  7.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:01<00:01,  8.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:00, 10.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 12.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 13.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 15.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.177771487236023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20 / 30] \t Step [5 / 25] \t Train Loss: 2.5708303928375242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.05it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20 / 30] \t Step [10 / 25] \t Train Loss: 2.386309027671814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20 / 30] \t Step [15 / 25] \t Train Loss: 1.9761330127716064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:00,  5.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20 / 30] \t Step [20 / 25] \t Train Loss: 2.4089226007461546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20 / 30] \t Step [25 / 25] \t Train Loss: 2.098223853111267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:04,  4.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  5.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:02,  7.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:01<00:01,  8.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01,  9.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 10.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 12.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 13.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 15.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 25/25 [00:01<00:00, 16.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.1108292531967163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:05<00:20,  5.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:05<00:10,  3.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:05<00:05,  2.59s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:05<00:01,  1.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:05<00:00,  1.36s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.62it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21 / 30] \t Step [5 / 25] \t Train Loss: 2.271582078933716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.45it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21 / 30] \t Step [10 / 25] \t Train Loss: 2.3536291122436523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21 / 30] \t Step [15 / 25] \t Train Loss: 2.147092652320862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.60it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21 / 30] \t Step [20 / 25] \t Train Loss: 1.8639814138412476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21 / 30] \t Step [25 / 25] \t Train Loss: 2.2050825119018556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 2/25 [00:00<00:08,  2.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 4/25 [00:00<00:05,  3.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:03,  4.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  6.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:01<00:01,  7.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:01<00:01,  9.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01, 10.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 12.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 19/25 [00:01<00:00, 14.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 15.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 16.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.100318078994751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22 / 30] \t Step [5 / 25] \t Train Loss: 2.201532316207886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22 / 30] \t Step [10 / 25] \t Train Loss: 2.0687973260879517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22 / 30] \t Step [15 / 25] \t Train Loss: 2.336742305755615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.00it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22 / 30] \t Step [20 / 25] \t Train Loss: 1.822064709663391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22 / 30] \t Step [25 / 25] \t Train Loss: 1.9058581113815307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:08,  2.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  6.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:01,  7.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  9.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:00, 11.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 12.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 14.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 16.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.0862687683105468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23 / 30] \t Step [5 / 25] \t Train Loss: 2.126802659034729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:00,  5.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.64it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23 / 30] \t Step [10 / 25] \t Train Loss: 1.8066803693771363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.32it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23 / 30] \t Step [15 / 25] \t Train Loss: 1.8298020482063293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.02it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23 / 30] \t Step [20 / 25] \t Train Loss: 1.9738752365112304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23 / 30] \t Step [25 / 25] \t Train Loss: 2.044087862968445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  5.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 11/25 [00:00<00:01,  7.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:00<00:01,  9.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 11.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 12.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 14.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 16.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.0321297788619996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.34it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24 / 30] \t Step [5 / 25] \t Train Loss: 1.8956607103347778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24 / 30] \t Step [10 / 25] \t Train Loss: 1.652863621711731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.28it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24 / 30] \t Step [15 / 25] \t Train Loss: 1.7465685606002808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24 / 30] \t Step [20 / 25] \t Train Loss: 2.092726993560791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24 / 30] \t Step [25 / 25] \t Train Loss: 1.8242928028106689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:08,  2.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:03,  4.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 8/25 [00:00<00:02,  6.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:01,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  9.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:00, 11.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 12.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 19/25 [00:01<00:00, 14.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 22/25 [00:01<00:00, 16.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 25/25 [00:01<00:00, 17.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.0275968980789183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25 / 30] \t Step [5 / 25] \t Train Loss: 1.7336055517196656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25 / 30] \t Step [10 / 25] \t Train Loss: 1.6490951299667358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25 / 30] \t Step [15 / 25] \t Train Loss: 2.031808042526245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25 / 30] \t Step [20 / 25] \t Train Loss: 1.8268418788909913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25 / 30] \t Step [25 / 25] \t Train Loss: 1.4572893857955933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:08,  2.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 2/25 [00:00<00:06,  3.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 4/25 [00:00<00:04,  4.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  5.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:01,  7.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:01<00:01,  9.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:00, 11.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 13.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 14.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 16.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.0410311365127565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26 / 30] \t Step [5 / 25] \t Train Loss: 1.6956332683563233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26 / 30] \t Step [10 / 25] \t Train Loss: 1.4325454831123352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26 / 30] \t Step [15 / 25] \t Train Loss: 1.8153404951095582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26 / 30] \t Step [20 / 25] \t Train Loss: 1.6442843198776245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26 / 30] \t Step [25 / 25] \t Train Loss: 1.6934292793273926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:08,  2.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 2/25 [00:00<00:06,  3.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 4/25 [00:00<00:04,  4.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 6/25 [00:00<00:03,  6.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  7.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 11/25 [00:00<00:01,  9.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:01<00:01, 11.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 15/25 [00:01<00:00, 12.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 14.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 15.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 17.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.0493170022964478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.28it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [27 / 30] \t Step [5 / 25] \t Train Loss: 1.24499169588089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [27 / 30] \t Step [10 / 25] \t Train Loss: 1.469376540184021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.28it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [27 / 30] \t Step [15 / 25] \t Train Loss: 1.6491044998168944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.96it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [27 / 30] \t Step [20 / 25] \t Train Loss: 1.8960464715957641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [27 / 30] \t Step [25 / 25] \t Train Loss: 1.475495743751526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  5.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:02,  7.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:00<00:01,  8.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01, 10.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17/25 [00:01<00:00, 12.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 20/25 [00:01<00:00, 14.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23/25 [00:01<00:00, 15.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.061821928024292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [28 / 30] \t Step [5 / 25] \t Train Loss: 1.5232759475708009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [28 / 30] \t Step [10 / 25] \t Train Loss: 1.2793728351593017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [28 / 30] \t Step [15 / 25] \t Train Loss: 1.4875619888305665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [28 / 30] \t Step [20 / 25] \t Train Loss: 1.3128705263137816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [28 / 30] \t Step [25 / 25] \t Train Loss: 1.7189233541488647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  5.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10/25 [00:00<00:02,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:01<00:01,  8.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01, 10.14it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 11.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 13.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 15.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 16.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.0719117069244386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.85it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [29 / 30] \t Step [5 / 25] \t Train Loss: 1.43756263256073\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [29 / 30] \t Step [10 / 25] \t Train Loss: 1.3553614497184754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [29 / 30] \t Step [15 / 25] \t Train Loss: 1.2672502160072328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.87it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [29 / 30] \t Step [20 / 25] \t Train Loss: 1.3683847069740296\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [29 / 30] \t Step [25 / 25] \t Train Loss: 1.4717206239700318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:09,  2.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  5.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  7.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 11/25 [00:00<00:01,  9.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 13/25 [00:01<00:01, 10.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 12.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18/25 [00:01<00:00, 14.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 21/25 [00:01<00:00, 15.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 24/25 [00:01<00:00, 17.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.122143516540527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:02,  1.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  2.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [30 / 30] \t Step [5 / 25] \t Train Loss: 1.1225677967071532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [30 / 30] \t Step [10 / 25] \t Train Loss: 1.3153573274612427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [30 / 30] \t Step [15 / 25] \t Train Loss: 1.4281273126602172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.80it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [30 / 30] \t Step [20 / 25] \t Train Loss: 1.2505608201026917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                     \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [30 / 30] \t Step [25 / 25] \t Train Loss: 1.266396450996399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 1/25 [00:00<00:10,  2.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3/25 [00:00<00:06,  3.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 5/25 [00:00<00:04,  4.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7/25 [00:00<00:03,  5.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 9/25 [00:00<00:02,  6.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12/25 [00:01<00:01,  8.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 14/25 [00:01<00:01, 10.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16/25 [00:01<00:00, 12.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 19/25 [00:01<00:00, 14.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 22/25 [00:01<00:00, 15.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 25/25 [00:01<00:00, 17.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                               \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Loss: 3.158693780899048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHPlROJkez-0",
        "outputId": "93a09b48-cbaa-49f3-b88a-c4508c3632dc"
      },
      "source": [
        "print(len(train_losses))\n",
        "print(len(val_losses))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "idb6HTiKje2s",
        "outputId": "a2f4d83d-7f26-46b3-d698-2a1f5e6fb037"
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.title(\"Train-Validation Loss\")\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='validation')\n",
        "\n",
        "plt.xlabel('num_epochs', fontsize=12)\n",
        "plt.ylabel('loss', fontsize=12)\n",
        "plt.legend(loc='best')\n",
        "print()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}